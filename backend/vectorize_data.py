"""
ë°ì´í„° ë²¡í„°í™” ë° FAISS ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ìˆ˜ì—…ê³„íšì„œ JSON ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""
import json
import os
import pickle
from pathlib import Path
from typing import List, Dict, Any
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm


class DataVectorizer:
    """ë°ì´í„° ë²¡í„°í™” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì§€ì› ëª¨ë¸)
        """
        print(f"ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.dimension})")
        
    def extract_text_from_json(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        JSON ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            data: ìˆ˜ì—…ê³„íšì„œ JSON ë°ì´í„°
            
        Returns:
            í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        chunks = []
        
        # _metadataëŠ” ê±´ë„ˆëœ€
        for subject_name, subject_data in data.items():
            if subject_name == "_metadata":
                continue
                
            # êµê³¼ëª© ìš´ì˜ ì •ë³´
            if "êµê³¼ëª© ìš´ì˜" in subject_data:
                ìš´ì˜ì •ë³´ = subject_data["êµê³¼ëª© ìš´ì˜"]
                text = f"""êµê³¼ëª©: {ìš´ì˜ì •ë³´.get('êµê³¼ëª©', '')}
ë‹´ë‹¹êµìˆ˜: {ìš´ì˜ì •ë³´.get('ë‹´ë‹¹êµìˆ˜', '')}
ì´ìˆ˜êµ¬ë¶„: {ìš´ì˜ì •ë³´.get('ì´ìˆ˜êµ¬ë¶„', '')}
ì‹œê°„/í•™ì : {ìš´ì˜ì •ë³´.get('ì‹œê°„/í•™ì ', '')}
ì´ë¡ /ì‹¤ìŠµ: {ìš´ì˜ì •ë³´.get('ì´ë¡ /ì‹¤ìŠµ', '')}
ì—°ë½ì²˜: {ìš´ì˜ì •ë³´.get('ì—°ë½ì²˜', '')}
ì´ë©”ì¼: {ìš´ì˜ì •ë³´.get('E-Mail', '')}"""
                
                chunks.append({
                    "text": text,
                    "metadata": {
                        "subject": subject_name,
                        "type": "êµê³¼ëª©_ìš´ì˜",
                        "professor": ìš´ì˜ì •ë³´.get('ë‹´ë‹¹êµìˆ˜', '')
                    }
                })
            
            # êµê³¼ëª© ê°œìš”
            if "êµê³¼ëª© ê°œìš”" in subject_data:
                ê°œìš” = subject_data["êµê³¼ëª© ê°œìš”"]
                text_parts = []
                
                for key, value in ê°œìš”.items():
                    if value and value != "NaN" and not isinstance(value, dict):
                        text_parts.append(f"{key}: {value}")
                
                if text_parts:
                    chunks.append({
                        "text": "\n".join(text_parts),
                        "metadata": {
                            "subject": subject_name,
                            "type": "êµê³¼ëª©_ê°œìš”"
                        }
                    })
            
            # ìˆ˜ì—…ê³„íš (ì£¼ì°¨ë³„)
            if "ìˆ˜ì—…ê³„íš" in subject_data:
                ìˆ˜ì—…ê³„íš = subject_data["ìˆ˜ì—…ê³„íš"]
                for week, week_data in ìˆ˜ì—…ê³„íš.items():
                    if isinstance(week_data, dict):
                        text = f"""ì£¼ì°¨: {week}
ìˆ˜ì—…ì£¼ì œ ë° ë‚´ìš©: {week_data.get('ìˆ˜ì—…ì£¼ì œ ë° ë‚´ìš©', '')}
ìˆ˜ì—…ë°©ë²•: {week_data.get('ìˆ˜ì—…ë°©ë²•', '')}
í•™ìƒì„±ì¥(ì—­ëŸ‰ì œê³ ) ì „ëµ: {week_data.get('í•™ìƒì„±ì¥(ì—­ëŸ‰ì œê³ ) ì „ëµ', '')}"""
                        
                        chunks.append({
                            "text": text,
                            "metadata": {
                                "subject": subject_name,
                                "type": "ìˆ˜ì—…ê³„íš",
                                "week": week
                            }
                        })
            
            # í‰ê°€ê°œìš”
            if "í‰ê°€ê°œìš”" in subject_data:
                í‰ê°€ê°œìš” = subject_data["í‰ê°€ê°œìš”"]
                for eval_type, eval_data in í‰ê°€ê°œìš”.items():
                    if isinstance(eval_data, dict):
                        text = f"í‰ê°€ìœ í˜•: {eval_type}\ní‰ê°€ë‚´ìš©: {eval_data.get('í‰ê°€ë‚´ìš©', '')}"
                        chunks.append({
                            "text": text,
                            "metadata": {
                                "subject": subject_name,
                                "type": "í‰ê°€ê°œìš”"
                            }
                        })
        
        print(f"ğŸ“„ ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œë¨")
        return chunks
    
    def vectorize_chunks(self, chunks: List[Dict[str, Any]]) -> tuple:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ë²¡í„° ë°°ì—´, ì²­í¬ ë¦¬ìŠ¤íŠ¸)
        """
        print("ğŸ”„ í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
        texts = [chunk["text"] for chunk in chunks]
        
        # ë°°ì¹˜ë¡œ ì¸ì½”ë”© (ì§„í–‰ë¥  í‘œì‹œ)
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"âœ… ë²¡í„°í™” ì™„ë£Œ: {embeddings.shape}")
        return embeddings, chunks
    
    def create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """
        FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            embeddings: ë²¡í„° ë°°ì—´
            
        Returns:
            FAISS ì¸ë±ìŠ¤
        """
        print("ğŸ—‚ï¸ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.IndexFlatL2(self.dimension)
        
        # ë²¡í„°ë¥¼ float32ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
        embeddings_float32 = embeddings.astype('float32')
        index.add(embeddings_float32)
        
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    def save_index(self, index: faiss.Index, chunks: List[Dict], output_dir: str):
        """
        FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            index: FAISS ì¸ë±ìŠ¤
            chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        index_file = output_path / "faiss_index.bin"
        faiss.write_index(index, str(index_file))
        print(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥: {index_file}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = output_path / "chunks_metadata.pkl"
        with open(metadata_file, "wb") as f:
            pickle.dump(chunks, f)
        print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")
        
        print(f"\nâœ… ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ!")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent
    data_file = project_root / "utils" / "output.json"
    output_dir = script_dir / "vector_db"
    
    print("=" * 60)
    print("ğŸš€ ìˆ˜ì—…ê³„íšì„œ ë°ì´í„° ë²¡í„°í™” ì‹œì‘")
    print("=" * 60)
    
    # 1. JSON ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {data_file}")
    if not data_file.exists():
        print(f"âŒ ì˜¤ë¥˜: {data_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë¨¼ì € utils/excel_utils.pyë¥¼ ì‹¤í–‰í•˜ì—¬ output.jsonì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    with open(data_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
    # 2. ë²¡í„°í™” ê°ì²´ ìƒì„±
    vectorizer = DataVectorizer()
    
    # 3. í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ
    chunks = vectorizer.extract_text_from_json(data)
    
    if not chunks:
        print("âŒ ì˜¤ë¥˜: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 4. ë²¡í„°í™”
    embeddings, chunks = vectorizer.vectorize_chunks(chunks)
    
    # 5. FAISS ì¸ë±ìŠ¤ ìƒì„±
    index = vectorizer.create_faiss_index(embeddings)
    
    # 6. ì €ì¥
    vectorizer.save_index(index, chunks, str(output_dir))
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ë²¡í„°í™” ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"  - ë²¡í„° ì°¨ì›: {vectorizer.dimension}")
    print(f"  - ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()

